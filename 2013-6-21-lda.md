Title: 使用LDA对用户在线评论进行分类
Tags: R
Date:2013-6-21


##采集数据

本文的数据来源于，淘宝上女性用户对文胸的评论。原谅哥没出息，对这一神秘物件充满了好奇。由于没给我广告费，我就不上链接了。然后采用[火车采集器][locoy]对用户评论抓取下来。一条评论用一个txt文档保存。还没学会用Python写爬虫，深感知识的贫乏，惭愧。

##分词处理

分词处理采用，[Rwordseg][wordseg]进行分词。

     
    library(Rwordseg)
    ## 对clothe文件下的400篇文档，进行分词
    for (i in 1:400){
        file.path <- paste("~/code/jss/clothe/", i, ".txt", sep = "")
        txt <- readLines(file.path, encoding = 'utf-8')
        ## 去掉空白行
        txt = txt[txt!=" "]
        ## 分词
        words = unlist(lapply(txt,segmentCN))
        ## 文件分词后保存在corpus目录下
        file.save <- paste("corpus/", i, ".txt", sep = "")
        write.table(words, file.save, fileEncoding = 'utf-8')
    }

##数据清理

分词后的文件，内容是杂乱的。可能是我走弯路了。都是这样的内容：

     
    "x"
    "1" "多次"
    "2" "购买"
    "3" "了"
    "4" "很"
    "5" "不错"
    "6" "就"
    "7" "是"
    "8" "稍微"
    "9" "有点"
    "10" "厚"

为了将这样的文档格式改为这样：

     
    多次 购买 了 很 不错 就 是 稍微 有点 厚

我采用了python的正则表达式进行替换，python代码简单精炼，装X地说：

> code is poetry

以下是代码：

    :::python
    #!/usr/bin/python
    # coding=utf-8   #
    import os
    import re
    import codecs

    #更改工作目录
    os.chdir('/home/max/code/jss/corpus')

    i = 0

    for file in os.listdir('.'):
        ## 读取目录下的文件
        raw_file = codecs.open(file,'r','utf-8')
        text = raw_file.read()
        ## 去掉文件中所有的数字，双引号，还有回车符"\n"
        word = re.sub(r'[\d"x\n]','',text)
        ## 保存文件的目录是在jss目录子目录clean下
        wellfilename = '/home/max/code/jss/clean/'+ str(i) + '.txt'
        wellfile = codecs.open(wellfilename, 'w', 'utf-8')
        i = i + 1
        wellfile.write(word)
        raw_file.close()

## 使用tm包

分词以后，使用R语言中的[tm][text minig]包，生成`DocumentTermMatrix`。

     
    library(tm)
    #生成语料库
    cor.path <- "clean"
    cor <- Corpus(DirSource(directory = cor.path, encoding = "UTF-8"))
    ## 对语料库进行去除空白，标点，数字操作
    ## 其实在此例中，没必要，因为的输入文本非常规矩
    ## 已经用Python清洗过了
    cor.cl <- tm_map(cor,stripWhitespace)
    cor.cl <- tm_map(cor.cl,removePunctuation)
    cor.cl <- tm_map(cor.cl,removeNumbers)
    ## 加载停止词
    mystopwords <- readLines("stopwords.txt",encoding = "UTF-8")
    ## 生成DocumentTermMatrix
    cor.dtm <- DocumentTermMatrix(cor.cl, control = list(wordLengths = c(2, Inf),stopwords = mystopwords)

此时得到的`DocumentTermMatrix`是`高维稀疏矩阵`，有必要去除一些低频率的词。

     
    cor.dtm <- removeSparseTerms(cor.dtm, 0.99)
    ## 使得每一行至少有一个词不为0
    rowTotals <- apply(cor.dtm, 1, sum)
    cor.dtm <- cor.dtm[rowTotals > 0]

##使用topicmodels包
经过大瘦身以后，得到的`DocumentTermMatrix`可以作为`LDA`的实现包`topicmodels`使用。

     
    ## 用前250份文档进行学习
    result_LDA <- LDA(cor.dtm[1:250,],control = list(alpha = 0.1), k = 3)
    ## 后150份文档进行验证
    post <- posterior(result_LDA, newdata = cor.dtm[-c(1:150),])
    round(post$topics[1:5,],digits = 2)
    get_terms(result_LDA, 5)

结果为如下：

     
         Topic 1 Topic 2 Topic 3
    [1,] "不错"  "喜欢"  "合适" 
    [2,] "舒服"  "满意"  "罩杯" 
    [3,] "效果"  "舒服"  "内衣" 
    [4,] "质量"  "东西"  "舒适" 
    [5,] "聚拢"  "尺码"  "感觉" 


字眼是相当的销魂，可惜聚类以后不懂有什么作用。各位看官，今天到此为止吧。










[wordseg]: http://jliblog.com/app/rwordseg
[locoy]: http://www.locoy.com/ "火车采集器"
